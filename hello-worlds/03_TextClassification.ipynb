{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify positive/negative reviews\n",
    "----\n",
    "\n",
    "This notebook shows an example to classify text reviews from IMDB ([Internet Movie DataBase](https://www.imdb.com/)) are positive or negative review. This notebook is taken and modified from the [TensorFlow guide](https://www.tensorflow.org/tutorials/keras/basic_text_classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from datasets import imdb\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes from the TensorFlow package and it has been pre-processed that the reviews (sequence of words) have been converted into unique integers for each word. Labels are either 0 (negative review) or 1 (positive review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new reserved keywords\n",
    "\n",
    "Each sample contains a text that can be any number of words, but the network needs a fixed number of elements for each sample. Therefore, we need to somehow pad them with a unique reserved index (not a word).\n",
    "\n",
    "We need to create new indices for some non-word items: *pad*, *start*, *unknown* and *unused*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word dictionary for adding some new keywords\n",
    "word_dict = imdb.load_word_index()\n",
    "\n",
    "# shift the first four indices\n",
    "word_dict = {k:(v+3) for (k,v) in word_dict.items()}\n",
    "\n",
    "# make the first four indices to our new reserved non-word items\n",
    "word_dict['<PAD>'] = 0\n",
    "word_dict['<START>'] = 1\n",
    "word_dict['<UNKNOWN>'] = 2\n",
    "word_dict['<UNUSED>'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the number of elements from all samples equal\n",
    "\n",
    "Load the data and pad each sample with the new `<PAD>` keywords at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: data = (25000, 256), labels = (25000,)\n",
      "Test:     data = (25000, 256), labels = (25000,)\n"
     ]
    }
   ],
   "source": [
    "# To make the size manageable, we filter the top 10,000 most frequent words in the training data.\n",
    "# Hence, rare words are not being trained.\n",
    "num_words = 10000\n",
    "\n",
    "# maximum length of the review for each sample\n",
    "# use None for getting to max, but that may create different size between training & test data\n",
    "max_sample = 256\n",
    "\n",
    "# load the data\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load(num_words=num_words)\n",
    "\n",
    "# pad training and test data with <PAD> at the end of each sample, \n",
    "# such that all samples have equal number of elements\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                       value = word_dict['<PAD>'],\n",
    "                                                       padding = 'post',\n",
    "                                                       maxlen = max_sample)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value = word_dict['<PAD>'],\n",
    "                                                       padding = 'post',\n",
    "                                                       maxlen = max_sample)\n",
    "\n",
    "# let's print the shape of our data\n",
    "print(\"Training: data = {}, labels = {}\".format(train_data.shape, train_labels.shape))\n",
    "print(\"Test:     data = {}, labels = {}\".format(test_data.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "We can create a function to decode the word indices and print the original text of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get reverse word indices based on our modified word indices\n",
    "rev_word_index = imdb.get_rev_word_index(word_dict)\n",
    "\n",
    "def decode_review(indices):\n",
    "    return \" \".join([rev_word_index[i] for i in indices if i>3]).strip()\n",
    "\n",
    "# test one\n",
    "decode_review(train_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "----\n",
    "\n",
    "The input data is an array of integers (word indices). The labels are 0 or 1. The first layer to create is an *embedding layer*, which creates a dense vector from the sparsed word indices. The dense vectors are learned during training.\n",
    "\n",
    "For example, if the sentence is \"deep learning is really deep to learn\", then the word indices are: `[0 1 2 3 1 4 5]`, where the dictionary is\n",
    "```\n",
    "[0, deep]\n",
    "[1, learning]\n",
    "[2, is]\n",
    "[3, really]\n",
    "[4, to]\n",
    "[5, learn]\n",
    "```\n",
    "The embedding layer transform it into a real value vector of `D` dimension for each index. The size of `D` is free to choose. Hence, embedding layer actually converts discrete space input into continuous domain.\n",
    "\n",
    "At the beginning, the values of these vectors are chosen randomly. During the training, the network will learn, and it is usually based on the words adjacent to it, distance or nearest neighbor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers are:\n",
    "\n",
    "* [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) with size of `feature_dim`.\n",
    "* [GlobalAveragePooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) to take the average value of the embedded vector, \n",
    "* [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), a fully connected layer from `feature_dim` neurons with [relu](https://www.tensorflow.org/api_docs/python/tf/nn/relu) activation, and \n",
    "* The final [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) with [sigmoid](https://www.tensorflow.org/api_docs/python/tf/nn/relu) function for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 16)          272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 1)           17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_dim = 16\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(num_words, feature_dim))\n",
    "model.add(keras.layers.AveragePooling1D())\n",
    "model.add(keras.layers.Dense(feature_dim, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the model with [Adam optmizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and `binary_crossentropy` loss function. This loss function calculates the distance betweent two probability distribution functions, which is suitable for this classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Usually the training data is split into into training & validation set, randomly. Just to make it easier, we split the first 10000 samples as our validation data, and the model is trained on the remaining ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have 3 dimensions, but got array with shape (15000, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e81848de3ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/PY365ENV/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PY365ENV/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    991\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 993\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PY365ENV/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PY365ENV/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    314\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have 3 dimensions, but got array with shape (15000, 1)"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data[10000:],\n",
    "    train_labels[10000:],\n",
    "    epochs=40,\n",
    "    batch_size=512,\n",
    "    validation_data=(train_data[:10000], train_labels[:10000]),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
